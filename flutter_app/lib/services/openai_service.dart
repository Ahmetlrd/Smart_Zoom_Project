import 'dart:convert';
import 'dart:io';
import 'package:http/http.dart' as http;
import 'package:flutter_dotenv/flutter_dotenv.dart';

class OpenAIService {
  final String _apiKey = dotenv.env['smartZoomOpenAIKey'] ?? '';

  Future<String?> summarizeText(String text) async {
    final url = Uri.parse("https://api.openai.com/v1/chat/completions");

    final response = await http.post(
      url,
      headers: {
        'Authorization': 'Bearer $_apiKey',
        'Content-Type': 'application/json',
      },
      body: jsonEncode({
        "model": "gpt-4",
        "messages": [
          {
            "role": "system",
            "content":
                "You are a helpful assistant that summarizes transcripts."
          },
          {
            "role": "user",
            "content": """
AÅŸaÄŸÄ±da bir Zoom toplantÄ±sÄ±nÄ±n transkripti bulunmaktadÄ±r.

LÃ¼tfen bu transkripti analiz ederek Ã¶nce **toplantÄ±nÄ±n baÅŸlÄ±ÄŸÄ±** olacak ÅŸekilde ilk 1 satÄ±rÄ± Ã¼ret. TÄ±rnak kullanma. Bu baÅŸlÄ±k, transkriptten yola Ã§Ä±karak oluÅŸturulmalÄ± ve 5 kelime olacak ÅŸekilde iÃ§eriÄŸi yansÄ±tmalÄ±dÄ±r. BaÅŸlÄ±k satÄ±rÄ± ÅŸu formatta baÅŸlamalÄ±dÄ±r:

"Title: ..."

Bu satÄ±rdan sonra **asÄ±l Ã¶zete** geÃ§.
AÅŸaÄŸÄ±da bir Zoom toplantÄ±sÄ±nÄ±n transkripti bulunmaktadÄ±r.

Bu transkripti incele ve toplantÄ±nÄ±n iÃ§eriÄŸini profesyonelce Ã¶zetle.  
KullanÄ±cÄ± bu Ã¶zeti okuduÄŸunda toplantÄ±ya katÄ±lmadan ne konuÅŸulduÄŸunu net ÅŸekilde anlayabilsin. ToplantÄ±yÄ± tekrar izlemeye ihtiyaÃ§ duymasÄ±n.

Ã–zette ÅŸu baÅŸlÄ±klarÄ± mutlaka ele al:

- ToplantÄ±nÄ±n amacÄ± ve ana gÃ¼ndemi
- KonuÅŸan kiÅŸi(ler) kimlerdi? (isim varsa belirt)
- GÃ¶rÃ¼ÅŸÃ¼len Ã¶nemli konular, sorunlar, fikirler
- AlÄ±nan kararlar ve varÄ±lan sonuÃ§lar
- Eylem maddeleri (kim, ne zaman, ne yapacak)
- Dikkat Ã§eken ifadeler veya Ã¶nemli vurgular

Ã–zeti TÃ¼rkÃ§e yaz ve sade, akÄ±cÄ± paragraflarla sun.  
Maddeleme gerekiyorsa yap ama yapay hissettirmesin.  
Bilgi eksikse "bilgi eksik" deme, olanÄ± Ã¶zetle.

TRANSKRÄ°PT:
$text
"""
          }
        ],
        "temperature": 0.5,
      }),
    );

    if (response.statusCode == 200) {
      final decoded = jsonDecode(response.body);
      return decoded["choices"][0]["message"]["content"];
    } else {
      print("OpenAI error: ${response.body}");
      return null;
    }
  }

  Future<String?> transcribeAudio(File audioFile, {String? forceLanguage}) async {
  final url = Uri.parse("https://api.openai.com/v1/audio/transcriptions");

  final request = http.MultipartRequest('POST', url)
    ..headers['Authorization'] = 'Bearer $_apiKey'
    ..fields['model'] = 'whisper-1'
    ..fields['response_format'] = 'verbose_json'
    ..files.add(await http.MultipartFile.fromPath('file', audioFile.path));

  if (forceLanguage != null) {
    request.fields['language'] = forceLanguage;
  }

  final response = await request.send();

  if (response.statusCode == 200) {
    final responseBody = await response.stream.bytesToString();
    final json = jsonDecode(responseBody);
    print("ğŸ—£ï¸ Whisper algÄ±ladÄ±ÄŸÄ± dil: ${json['language']}");
    return json['text'];
  } else {
    final error = await response.stream.bytesToString();
    print("Whisper API error: $error");
    return null;
  }
}


}
